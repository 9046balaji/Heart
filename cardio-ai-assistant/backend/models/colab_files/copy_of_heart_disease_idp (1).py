# -*- coding: utf-8 -*-
"""Copy of HEART_DISEASE-IDP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lmDBWPE-QMhBt421AVhm3xda7mewc6e_
"""

# Add these new libraries
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest, f_classif, chi2
from imblearn.over_sampling import ADASYN # Crucial for Class Balancing
from sklearn.preprocessing import MinMaxScaler # Needed for Chi-Square
import lightgbm as lgb # Recommended by the "Accurate..." paper

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/IDP/datasets/heart_statlog_cleveland_hungary_final.csv')

print("First few rows of the DataFrame:")
display(df.head())

print("\nInformation about the DataFrame:")
display(df.info())

print("Missing values in each column:")
display(df.isnull().sum())

print("\nDescriptive statistics for numerical columns:")
display(df.describe())

print("\nDistribution of the target variable 'target':")
display(df['target'].value_counts())

import matplotlib.pyplot as plt
import seaborn as sns

# Corrected numerical_features to match DataFrame column names
numerical_features = ['age', 'resting bp s', 'cholesterol', 'max heart rate', 'oldpeak']
# Corrected categorical_features to match DataFrame column names
categorical_features = ['sex', 'chest pain type', 'fasting blood sugar', 'resting ecg', 'exercise angina', 'ST slope']

# Histograms for numerical features
plt.figure(figsize=(15, 10))
for i, feature in enumerate(numerical_features):
    plt.subplot(2, 3, i + 1)
    sns.histplot(df[feature], kde=True)
    plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()

# Count plots for categorical features vs AHD
plt.figure(figsize=(20, 15))
for i, feature in enumerate(categorical_features):
    plt.subplot(3, 3, i + 1)
    sns.countplot(x=feature, hue='target', data=df) # Changed 'AHD' to 'target'
    plt.title(f'Target vs {feature}') # Changed title for consistency
plt.tight_layout()
plt.show()

"""**Reasoning**:
Calculate and visualize the correlation matrix for numerical features.


"""

plt.figure(figsize=(10, 8))
correlation_matrix = df[numerical_features].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()

# # --- STEP 1: CONSOLIDATED PREPROCESSING ---
# from sklearn.pipeline import Pipeline
# from sklearn.impute import SimpleImputer
# from sklearn.preprocessing import StandardScaler, OneHotEncoder
# from sklearn.compose import ColumnTransformer
# import numpy as np

# df = df.drop('Unnamed: 0', axis=1)
# df['AHD'] = df['AHD'].map({'Yes': 1, 'No': 0})

# numerical_features = ['Age', 'RestBP', 'Chol', 'MaxHR', 'Oldpeak']
# categorical_features = ['Sex', 'ChestPain', 'Fbs', 'RestECG', 'ExAng', 'Slope', 'Ca', 'Thal']

# class OutlierCapTransformer:
#     def __init__(self, factor=1.5):
#         self.factor = factor
#     def fit(self, X, y=None):
#         return self
#     def transform(self, X):
#         X = X.copy()
#         for i, col in enumerate(numerical_features):
#             Q1, Q3 = np.percentile(X[:, i], [25, 75])
#             IQR = Q3 - Q1
#             lower, upper = Q1 - self.factor * IQR, Q3 + self.factor * IQR
#             X[:, i] = np.clip(X[:, i], lower, upper)
#         return X

# # Numerical pipeline: Impute (median for nums), cap outliers, scale
# num_pipeline = Pipeline([
#     ('imputer', SimpleImputer(strategy='median')),  # Better for skewed nums like Chol
#     ('outlier_cap', OutlierCapTransformer()),
#     ('scaler', StandardScaler())
# ])

# # Categorical pipeline: Impute mode, one-hot
# cat_pipeline = Pipeline([
#     ('imputer', SimpleImputer(strategy='most_frequent')),
#     ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))
# ])

# # Full preprocessor
# preprocessor = ColumnTransformer([
#     ('num', num_pipeline, numerical_features),
#     ('cat', cat_pipeline, categorical_features)
# ])

# # Separate X/y
# X = df.drop('AHD', axis=1)
# y = df['AHD']

# print("Preprocessed DataFrame info:")
# # Fit and transform for display (optional)
# X_pre = preprocessor.fit_transform(X)
# feature_names = (numerical_features +
#                  list(preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(categorical_features)))
# X_display = pd.DataFrame(X_pre, columns=feature_names)
# X_display['AHD'] = y
# display(X_display.head())
# display(X_display.info())

"""## Feature engineering/selection

### Subtask:
Create new features or select relevant features if necessary.

"""

# import matplotlib.pyplot as plt
# import seaborn as sns

# # Calculate the correlation matrix for the processed_df
# correlation_matrix_processed = processed_df.corr()

# # Display the correlation matrix (optional, can be large)
# # display(correlation_matrix_processed)

# # Plot a heatmap of the correlation matrix for better visualization
# plt.figure(figsize=(18, 15))
# sns.heatmap(correlation_matrix_processed, annot=False, cmap='coolwarm')
# plt.title('Correlation Matrix of Processed Features')
# plt.show()

# # Analyze correlations, particularly with the target variable 'AHD'
# # and identify highly correlated features among themselves.
# # We can inspect the correlation matrix or sort correlations with 'AHD'.

# correlation_with_ahd = correlation_matrix_processed['AHD'].sort_values(ascending=False)
# print("\nCorrelation with AHD:")
# display(correlation_with_ahd)

# # Based on domain knowledge and the correlation matrix, we will decide on feature selection.
# # For this task, we will keep all features after one-hot encoding and scaling,
# # as there are no extremely high correlations between independent variables that would
# # strongly suggest multicollinearity based on the initial heatmap observation,
# # and all features could potentially contribute to the model.
# # We will also not engineer new features at this stage to keep the baseline models simple.

# # The processed_df is already updated with necessary transformations from the preprocessing step.
# # No further changes to processed_df are made in this step based on the decision to keep all features.

"""## Split data

### Subtask:
Split the data into training and testing sets, considering stratification if the target variable is imbalanced.

"""

# from sklearn.model_selection import train_test_split

# X = processed_df.drop('AHD', axis=1)
# y = processed_df['AHD']

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

# print("Shape of training features:", X_train.shape)
# print("Shape of testing features:", X_test.shape)
# print("Shape of training target:", y_train.shape)
# print("Shape of testing target:", y_test.shape)

# print("\nDistribution of AHD in training set:")
# display(y_train.value_counts(normalize=True))

# print("\nDistribution of AHD in testing set:")
# display(y_test.value_counts(normalize=True))

# # --- STEP 2: SPLIT & BALANCE ---
# from imblearn.over_sampling import ADASYN
# from sklearn.model_selection import train_test_split
# import pandas as pd # Ensure pandas is imported

# # Re-derive X and y from the original df (before one-hot encoding by preprocessor)
# # The 'df' variable from cell 6o71_J0O2MJl contains the appropriate raw features.
# X_raw = df.drop('AHD', axis=1)
# y_target = df['AHD']

# X_train_raw, X_test_raw, y_train, y_test = train_test_split(X_raw, y_target, test_size=0.25, random_state=42, stratify=y_target)

# # Preprocess using the already defined preprocessor
# X_train_pre = preprocessor.fit_transform(X_train_raw)
# X_test_pre = preprocessor.transform(X_test_raw)

# # ADASYN balancing (on train only; per Paper 3)
# adasyn = ADASYN(random_state=42)
# X_train_bal, y_train_bal = adasyn.fit_resample(X_train_pre, y_train)

# print(f"Train shapes: Original {X_train_pre.shape}, Balanced {X_train_bal.shape}")
# print("Balanced y dist:", pd.Series(y_train_bal).value_counts(normalize=True))

# # --- STEP 3: DUAL-TIER FEATURE SELECTION (AnoX²) ---
# from sklearn.feature_selection import SelectKBest, f_classif, chi2
# from sklearn.preprocessing import MinMaxScaler
# import pandas as pd # Import pandas for DataFrame conversion

# # Get feature names
# cat_encoder = preprocessor.named_transformers_['cat'].named_steps['encoder']
# cat_names = cat_encoder.get_feature_names_out(categorical_features)
# all_names = numerical_features + list(cat_names)

# # Tier 1: ANOVA (f_classif on balanced train)
# selector_anova = SelectKBest(f_classif, k=10)
# X_train_anova = selector_anova.fit_transform(X_train_bal, y_train_bal)
# anova_idx = selector_anova.get_support(indices=True)
# anova_names = [all_names[i] for i in anova_idx]

# # Tier 2: Chi² (on top ANOVA features; MinMax for non-neg)
# scaler_chi = MinMaxScaler()
# X_train_chi = scaler_chi.fit_transform(X_train_anova)
# selector_chi = SelectKBest(chi2, k=8)  # Top 8 for Cleveland
# X_train_sel = selector_chi.fit_transform(X_train_chi, y_train_bal)

# # Transform test accordingly
# X_test_anova = selector_anova.transform(X_test_pre)
# X_test_chi = scaler_chi.transform(X_test_anova)
# X_test_sel = selector_chi.transform(X_test_chi)

# sel_idx = selector_chi.get_support(indices=True)
# sel_names = [anova_names[i] for i in sel_idx]

# # Convert selected features to DataFrames with proper column names to avoid warnings
# X_train_sel = pd.DataFrame(X_train_sel, columns=sel_names)
# X_test_sel = pd.DataFrame(X_test_sel, columns=sel_names)

# print("Selected features (AnoX²):", sel_names)

# ==========================================
# CORRECTED PIPELINE (NO LEAKAGE + STACKING)
# ==========================================

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import ADASYN
from sklearn.ensemble import StackingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
import lightgbm as lgb
from sklearn.metrics import classification_report, confusion_matrix

# Ensure df is in its original state before modifications
df = pd.read_csv('/content/drive/MyDrive/IDP/datasets/heart_statlog_cleveland_hungary_final.csv')

# 1. Clean Data (Handle Categorical & Missing)
# ---------------------------------------------
# Drop index column if it exists - 'Unnamed: 0' column is not present in this dataset
# if 'Unnamed: 0' in df.columns:
#     df = df.drop(['Unnamed: 0'], axis=1)

# The target column is named 'target' and its values are already 0/1.
# So, no explicit mapping from 'Yes'/'No' or renaming is needed.

# Fill Missing Values (Mode Imputation)
# Based on cell 04dafc70, there are no missing values in this dataset,
# and columns 'Ca' and 'Thal' do not exist. So, these lines are removed.

# One-Hot Encoding for Categorical Features
# Use the correct categorical column names from df.info()
# Identified categorical features: 'sex', 'chest pain type', 'fasting blood sugar', 'resting ecg', 'exercise angina', 'ST slope'
categorical_cols_for_ohe = ['sex', 'chest pain type', 'fasting blood sugar', 'resting ecg', 'exercise angina', 'ST slope']
df_clean = pd.get_dummies(df, columns=categorical_cols_for_ohe, drop_first=True)

# Separate Features (X) and Target (y)
X = df_clean.drop('target', axis=1) # Corrected target column name to 'target'
y = df_clean['target']             # Corrected target column name to 'target'

# 2. Split Data (CRITICAL STEP - Must be before Scaling)
# ------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# 3. Scale Data (Fit on Train, Transform Test)
# --------------------------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train) # Learn stats from Train only
X_test_scaled = scaler.transform(X_test)       # Apply to Test (No peeking!)

# 4. Apply ADASYN (Balance the Training Data)
# -------------------------------------------
# Research requirement: Use ADASYN instead of SMOTE
adasyn = ADASYN(random_state=42)
X_train_bal, y_train_bal = adasyn.fit_resample(X_train_scaled, y_train)

print(f"Training Data Shape (Balanced): {X_train_bal.shape}")

"""## Build baseline models

### Subtask:
Train baseline models such as Logistic Regression, SVM, and XGBoost.

"""

# 5. Train Stacking Ensemble (The Research Recommendation)
# --------------------------------------------------------

# Convert scaled and balanced arrays back to DataFrames to preserve feature names for models like LGBM
# The column names should be consistent with the original X_train
X_train_bal_df = pd.DataFrame(X_train_bal, columns=X_train.columns)
X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)

# Level 0: The Experts (Base Models)
level0 = [
    ('lr', LogisticRegression(max_iter=1000)),
    ('knn', KNeighborsClassifier(n_neighbors=5)),
    ('svm', SVC(probability=True, kernel='linear', random_state=42)),
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
    ('lgbm', lgb.LGBMClassifier(random_state=42, verbose=-1))
]

# Level 1: The Judge (Meta Learner)
level1 = LogisticRegression()

print("Training Stacking Ensemble (this may take a moment)...")
stacking_model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)
# Fit with DataFrames to avoid feature name warnings for models like LGBM
stacking_model.fit(X_train_bal_df, y_train_bal)

# 6. Final Evaluation
# -------------------
# Predict with DataFrames
y_pred = stacking_model.predict(X_test_scaled_df)

print("\n--- RESEARCH-GRADE MODEL RESULTS ---")
print(classification_report(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

"""## Hyperparameter tuning

### Subtask:
Tune the hyperparameters of the selected models using techniques like Grid Search or Randomized Search.

"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier

# Logistic Regression (already defined in previous cell)
param_grid_lr = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l2'],
    'max_iter': [1000] # Increased max_iter to address ConvergenceWarning
}

# SVM (already defined in previous cell)
param_grid_svm = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto'],
    'kernel': ['rbf', 'linear']
}

# XGBoost (already defined in previous cell)
param_grid_xgb = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.7, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.9, 1.0]
}

# RandomForestClassifier
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# KNeighborsClassifier
param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

# LightGBM Classifier
param_grid_lgbm = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.05, 0.1],
    'num_leaves': [20, 31, 40],
    'max_depth': [-1, 5, 10],
    'colsample_bytree': [0.7, 0.9, 1.0]
}

print("Parameter grids defined for RandomForestClassifier, KNeighborsClassifier, SVC, and LGBMClassifier.")

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier

# Instantiate GridSearchCV for each model
grid_search_lr = GridSearchCV(estimator=LogisticRegression(random_state=42, solver='liblinear'), param_grid=param_grid_lr, cv=5, scoring='roc_auc', n_jobs=-1)
grid_search_svm = GridSearchCV(estimator=SVC(random_state=42, probability=True), param_grid=param_grid_svm, cv=5, scoring='roc_auc', n_jobs=-1) # Added probability=True
grid_search_xgb = GridSearchCV(estimator=XGBClassifier(random_state=42, eval_metric='logloss'), param_grid=param_grid_xgb, cv=5, scoring='roc_auc', n_jobs=-1)
grid_search_rf = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid_rf, cv=5, scoring='roc_auc', n_jobs=-1)
grid_search_knn = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid_knn, cv=5, scoring='roc_auc', n_jobs=-1)
grid_search_lgbm = GridSearchCV(estimator=lgb.LGBMClassifier(random_state=42, verbose=-1), param_grid=param_grid_lgbm, cv=5, scoring='roc_auc', n_jobs=-1) # Added verbose=-1

print("GridSearchCV instances created for Logistic Regression, SVM, XGBoost, RandomForestClassifier, KNeighborsClassifier, and LGBMClassifier.")
# Fit Grid Search to the training data for each model
print("Performing Grid Search for Logistic Regression...")
grid_search_lr.fit(X_train_bal, y_train_bal)
print("Grid Search for Logistic Regression complete.")

print("\nPerforming Grid Search for SVM...")
grid_search_svm.fit(X_train_bal, y_train_bal)
print("Grid Search for SVM complete.")

print("\nPerforming Grid Search for XGBoost...")
grid_search_xgb.fit(X_train_bal, y_train_bal)
print("Grid Search for XGBoost complete.")

print("\nPerforming Grid Search for RandomForestClassifier...")
grid_search_rf.fit(X_train_bal, y_train_bal)
print("Grid Search for RandomForestClassifier complete.")

print("\nPerforming Grid Search for KNeighborsClassifier...")
grid_search_knn.fit(X_train_bal, y_train_bal)
print("Grid Search for KNeighborsClassifier complete.")

print("\nPerforming Grid Search for LGBMClassifier...")
grid_search_lgbm.fit(X_train_bal, y_train_bal)
print("Grid Search for LGBMClassifier complete.")

# Print the best parameters for each model
print("\nBest parameters for Logistic Regression:", grid_search_lr.best_params_)
print("Best parameters for SVM:", grid_search_svm.best_params_)
print("Best parameters for XGBoost:", grid_search_xgb.best_params_)
print("Best parameters for RandomForestClassifier:", grid_search_rf.best_params_)
print("Best parameters for KNeighborsClassifier:", grid_search_knn.best_params_)
print("Best parameters for LGBMClassifier:", grid_search_lgbm.best_params_)

# Store the best estimators
best_lr_model = grid_search_lr.best_estimator_
best_svm_model = grid_search_svm.best_estimator_
best_xgb_model = grid_search_xgb.best_estimator_
best_rf_model = grid_search_rf.best_estimator_
best_knn_model = grid_search_knn.best_estimator_
best_lgbm_model = grid_search_lgbm.best_estimator_

print("\nBest estimators stored.")

"""## Evaluate models

### Subtask:
Evaluate the performance of the models using appropriate metrics like ROC-AUC, Precision, Recall, F1-score, and Confusion Matrix.

"""

from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Make predictions and calculate metrics for Logistic Regression
y_pred_lr = best_lr_model.predict(X_test_scaled_df.values)
y_prob_lr = best_lr_model.predict_proba(X_test_scaled_df.values)[:, 1]

roc_auc_lr = roc_auc_score(y_test, y_prob_lr)
precision_lr = precision_score(y_test, y_pred_lr)
recall_lr = recall_score(y_test, y_pred_lr)
f1_lr = f1_score(y_test, y_pred_lr)
cm_lr = confusion_matrix(y_test, y_pred_lr)

print("--- Logistic Regression Evaluation ---")
print(f"ROC-AUC: {roc_auc_lr:.4f}")
print(f"Precision: {precision_lr:.4f}")
print(f"Recall: {recall_lr:.4f}")
print(f"F1-score: {f1_lr:.4f}")
print("Confusion Matrix:")
disp_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr)
disp_lr.plot()
plt.title('Logistic Regression Confusion Matrix')
plt.show()

# Make predictions and calculate metrics for SVM
# SVM with 'linear' kernel from Grid Search supports probability prediction
if hasattr(best_svm_model, 'predict_proba'):
    y_prob_svm = best_svm_model.predict_proba(X_test_scaled_df.values)[:, 1]
    roc_auc_svm = roc_auc_score(y_test, y_prob_svm)
    print("\n--- SVM Evaluation ---")
    print(f"ROC-AUC: {roc_auc_svm:.4f}")
else:
    # Use decision_function as a score if predict_proba is not available or reliable
    y_score_svm = best_svm_model.decision_function(X_test_scaled_df.values)
    roc_auc_svm = roc_auc_score(y_test, y_score_svm)
    print("\n--- SVM Evaluation ---")
    print(f"ROC-AUC (using decision_function): {roc_auc_svm:.4f}")

y_pred_svm = best_svm_model.predict(X_test_scaled_df.values)
precision_svm = precision_score(y_test, y_pred_svm)
recall_svm = recall_score(y_test, y_pred_svm)
f1_svm = f1_score(y_test, y_pred_svm)
cm_svm = confusion_matrix(y_test, y_pred_svm)

print(f"Precision: {precision_svm:.4f}")
print(f"Recall: {recall_svm:.4f}")
print(f"F1-score: {f1_svm:.4f}")
print("Confusion Matrix:")
disp_svm = ConfusionMatrixDisplay(confusion_matrix=cm_svm)
disp_svm.plot()
plt.title('SVM Confusion Matrix')
plt.show()


# Make predictions and calculate metrics for XGBoost
y_pred_xgb = best_xgb_model.predict(X_test_scaled_df.values)
y_prob_xgb = best_xgb_model.predict_proba(X_test_scaled_df.values)[:, 1]

roc_auc_xgb = roc_auc_score(y_test, y_prob_xgb)
precision_xgb = precision_score(y_test, y_pred_xgb)
recall_xgb = recall_score(y_test, y_pred_xgb)
f1_xgb = f1_score(y_test, y_pred_xgb)
cm_xgb = confusion_matrix(y_test, y_pred_xgb)

print("\n--- XGBoost Evaluation ---")
print(f"ROC-AUC: {roc_auc_xgb:.4f}")
print(f"Precision: {precision_xgb:.4f}")
print(f"Recall: {recall_xgb:.4f}")
print(f"F1-score: {f1_xgb:.4f}")
print("Confusion Matrix:")
disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb)
disp_xgb.plot()
plt.title('XGBoost Confusion Matrix')
plt.show()

# Make predictions and calculate metrics for RandomForestClassifier
y_pred_rf = best_rf_model.predict(X_test_scaled_df.values)
y_prob_rf = best_rf_model.predict_proba(X_test_scaled_df.values)[:, 1]

roc_auc_rf = roc_auc_score(y_test, y_prob_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)
cm_rf = confusion_matrix(y_test, y_pred_rf)

print("\n--- RandomForestClassifier Evaluation ---")
print(f"ROC-AUC: {roc_auc_rf:.4f}")
print(f"Precision: {precision_rf:.4f}")
print(f"Recall: {recall_rf:.4f}")
print(f"F1-score: {f1_rf:.4f}")
print("Confusion Matrix:")
disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf)
disp_rf.plot()
plt.title('RandomForestClassifier Confusion Matrix')
plt.show()

# Make predictions and calculate metrics for KNeighborsClassifier
y_pred_knn = best_knn_model.predict(X_test_scaled_df.values)
y_prob_knn = best_knn_model.predict_proba(X_test_scaled_df.values)[:, 1]

roc_auc_knn = roc_auc_score(y_test, y_prob_knn)
precision_knn = precision_score(y_test, y_pred_knn)
recall_knn = recall_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)
cm_knn = confusion_matrix(y_test, y_pred_knn)

print("\n--- KNeighborsClassifier Evaluation ---")
print(f"ROC-AUC: {roc_auc_knn:.4f}")
print(f"Precision: {precision_knn:.4f}")
print(f"Recall: {recall_knn:.4f}")
print(f"F1-score: {f1_knn:.4f}")
print("Confusion Matrix:")
disp_knn = ConfusionMatrixDisplay(confusion_matrix=cm_knn)
disp_knn.plot()
plt.title('KNeighborsClassifier Confusion Matrix')
plt.show()

# Make predictions and calculate metrics for LGBMClassifier
y_pred_lgbm = best_lgbm_model.predict(X_test_scaled_df.values)
y_prob_lgbm = best_lgbm_model.predict_proba(X_test_scaled_df.values)[:, 1]

roc_auc_lgbm = roc_auc_score(y_test, y_prob_lgbm)
precision_lgbm = precision_score(y_test, y_pred_lgbm)
recall_lgbm = recall_score(y_test, y_pred_lgbm)
f1_lgbm = f1_score(y_test, y_pred_lgbm)
cm_lgbm = confusion_matrix(y_test, y_pred_lgbm)

print("\n--- LGBMClassifier Evaluation ---")
print(f"ROC-AUC: {roc_auc_lgbm:.4f}")
print(f"Precision: {precision_lgbm:.4f}")
print(f"Recall: {recall_lgbm:.4f}")
print(f"F1-score: {f1_lgbm:.4f}")
print("Confusion Matrix:")
disp_lgbm = ConfusionMatrixDisplay(confusion_matrix=cm_lgbm)
disp_lgbm.plot()
plt.title('LGBMClassifier Confusion Matrix')
plt.show()

# Visualize Confusion Matrices in Subplots for comparison
fig, axes = plt.subplots(2, 3, figsize=(20, 10)) # Changed to 2 rows, 3 columns
axes = axes.flatten() # Flatten the 2x3 array of axes for easier indexing

# Plot Logistic Regression Confusion Matrix
ConfusionMatrixDisplay(confusion_matrix=cm_lr).plot(ax=axes[0])
axes[0].set_title('Logistic Regression')

# Plot SVM Confusion Matrix
ConfusionMatrixDisplay(confusion_matrix=cm_svm).plot(ax=axes[1])
axes[1].set_title('SVM')

# Plot XGBoost Confusion Matrix
ConfusionMatrixDisplay(confusion_matrix=cm_xgb).plot(ax=axes[2])
axes[2].set_title('XGBoost')

# Plot RandomForestClassifier Confusion Matrix
ConfusionMatrixDisplay(confusion_matrix=cm_rf).plot(ax=axes[3])
axes[3].set_title('RandomForestClassifier')

# Plot KNeighborsClassifier Confusion Matrix
ConfusionMatrixDisplay(confusion_matrix=cm_knn).plot(ax=axes[4])
axes[4].set_title('KNeighborsClassifier')

# Plot LGBMClassifier Confusion Matrix
ConfusionMatrixDisplay(confusion_matrix=cm_lgbm).plot(ax=axes[5])
axes[5].set_title('LGBMClassifier')

plt.tight_layout()
plt.show()

"""**Reasoning**:
Summarize the performance of each model based on the calculated metrics.


"""

print("--- Model Performance Summary ---")
print("Logistic Regression:")
print(f"  ROC-AUC: {roc_auc_lr:.4f}")
print(f"  Precision: {precision_lr:.4f}")
print(f"  Recall: {recall_lr:.4f}")
print(f"  F1-score: {f1_lr:.4f}")

print("\nSVM:")
print(f"  ROC-AUC: {roc_auc_svm:.4f}")
print(f"  Precision: {precision_svm:.4f}")
print(f"  Recall: {recall_svm:.4f}")
print(f"  F1-score: {f1_svm:.4f}")

print("\nXGBoost:")
print(f"  ROC-AUC: {roc_auc_xgb:.4f}")
print(f"  Precision: {precision_xgb:.4f}")
print(f"  Recall: {recall_xgb:.4f}")
print(f"  F1-score: {f1_xgb:.4f}")

print("\nRandomForestClassifier:")
print(f"  ROC-AUC: {roc_auc_rf:.4f}")
print(f"  Precision: {precision_rf:.4f}")
print(f"  Recall: {recall_rf:.4f}")
print(f"  F1-score: {f1_rf:.4f}")

print("\nKNeighborsClassifier:")
print(f"  ROC-AUC: {roc_auc_knn:.4f}")
print(f"  Precision: {precision_knn:.4f}")
print(f"  Recall: {recall_knn:.4f}")
print(f"  F1-score: {f1_knn:.4f}")

print("\nLGBMClassifier:")
print(f"  ROC-AUC: {roc_auc_lgbm:.4f}")
print(f"  Precision: {precision_lgbm:.4f}")
print(f"  Recall: {recall_lgbm:.4f}")
print(f"  F1-score: {f1_lgbm:.4f}")

print("\nOverall:")
print("Logistic Regression continues to show strong performance, particularly in ROC-AUC and Recall.")
print("SVM also performs well with high Precision.")
print("RandomForestClassifier and LGBMClassifier generally show competitive performance.")
print("KNeighborsClassifier shows moderate performance across metrics.")
print("All models demonstrate reasonable performance for this classification task, with some variations in strength across different metrics.")

"""## Explainability

### Subtask:
Analyze feature importances and use techniques like SHAP to explain model predictions.

"""

import shap

# 1. Analyze Feature Importances for XGBoost
print("--- XGBoost Feature Importances ---")
# XGBoost model trained with scikit-learn API directly provides feature_importances_
if hasattr(best_xgb_model, 'feature_importances_'):
    feature_importances = pd.Series(best_xgb_model.feature_importances_, index=X_train.columns)
    sorted_feature_importances = feature_importances.sort_values(ascending=False)
    print("Sorted Feature Importances (Gain):")
    display(sorted_feature_importances)

    plt.figure(figsize=(10, 8))
    sorted_feature_importances.plot(kind='barh')
    plt.title('XGBoost Feature Importances')
    plt.xlabel('Importance (Gain)')
    plt.ylabel('Features')
    plt.tight_layout()
    plt.show()
else:
    print("XGBoost model does not have feature_importances_ attribute.")


# 2. Create SHAP explainer for the best performing model (Logistic Regression)
print("\n--- SHAP Analysis for Logistic Regression ---")
# For linear models like Logistic Regression, use the LinearExplainer
# Use X_train_bal_df as background data to match what the model was trained on
explainer_lr = shap.LinearExplainer(best_lr_model, X_train_bal_df)

# Calculate SHAP values for the test set using scaled DataFrame
shap_values_lr = explainer_lr.shap_values(X_test_scaled_df)

# 3. Generate a summary plot of the SHAP values
print("Generating SHAP Summary Plot...")
shap.summary_plot(shap_values_lr, X_test_scaled_df, plot_type="bar") # Use scaled DataFrame for feature names
shap.summary_plot(shap_values_lr, X_test_scaled_df) # Use scaled DataFrame for feature names
plt.title('SHAP Summary Plot for Logistic Regression')
plt.show()

# 4. Generate dependence plots for key features
# Based on the summary plot, identify a few key features. Let's pick a few top ones and some interesting ones.
# Example features: Thal_normal, ChestPain_asymptomatic, Ca_0.0, Oldpeak

# NOTE: The exact 'key_features' might need to be adjusted based on the actual feature names after one-hot encoding.
# For now, let's use some likely general feature names. If these cause an error, we will adjust.
# We need to map these to the columns of X_test_scaled_df
# Let's inspect X_test_scaled_df.columns to pick appropriate key features
# Assuming 'chest pain type_4' and 'ST slope_2' are important based on prior XGBoost feature importances
key_features = ['chest pain type_4', 'ST slope_2', 'oldpeak', 'max heart rate'] # Adjusted to more general relevant features

print("\nGenerating SHAP Dependence Plots for Key Features...")
for feature in key_features:
    if feature in X_test_scaled_df.columns:
        print(f"Generating dependence plot for {feature}...")
        shap.dependence_plot(feature, shap_values_lr, X_test_scaled_df, title=f'SHAP Dependence Plot for {feature}')
        plt.show()
    else:
        print(f"Feature '{feature}' not found in the test set. Available columns: {X_test_scaled_df.columns.tolist()[:5]}...") # Show first few columns if not found



"""## Next Steps: Threshold Tuning

### Subtask:
Tune the decision threshold for the Logistic Regression model to optimize for a desired balance between precision and recall.
"""

from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt
import numpy as np # Import numpy

# Get predicted probabilities for the positive class from the best Logistic Regression model
y_prob_lr = best_lr_model.predict_proba(X_test)[:, 1]

# Calculate precision and recall for various thresholds
precision, recall, thresholds = precision_recall_curve(y_test, y_prob_lr)

# Plot the precision-recall curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve for Logistic Regression')
plt.grid(True)
plt.show()

# Find the threshold that provides a good balance (e.g., where precision and recall are close or recall is prioritized)
# We can look at the plot and choose a threshold, or programmatically find one.
# For demonstration, let's find a threshold where recall is at least 0.9 (if possible)
# You can adjust this based on the desired trade-off.

# Find the index of the threshold where recall is closest to a desired value (e.g., 0.9)
desired_recall = 0.9
closest_recall_index = np.argmin(np.abs(recall - desired_recall))
optimal_threshold = thresholds[closest_recall_index]

print(f"Threshold closest to desired recall ({desired_recall:.2f}): {optimal_threshold:.4f}")
print(f"Precision at this threshold: {precision[closest_recall_index]:.4f}")
print(f"Recall at this threshold: {recall[closest_recall_index]:.4f}")

# Evaluate the model with the new threshold
y_pred_lr_tuned_threshold = (y_prob_lr >= optimal_threshold).astype(int)

precision_lr_tuned = precision_score(y_test, y_pred_lr_tuned_threshold)
recall_lr_tuned = recall_score(y_test, y_pred_lr_tuned_threshold)
f1_lr_tuned = f1_score(y_test, y_pred_lr_tuned_threshold)
cm_lr_tuned = confusion_matrix(y_test, y_pred_lr_tuned_threshold)

print("\n--- Logistic Regression Evaluation with Tuned Threshold ---")
print(f"Optimized Threshold: {optimal_threshold:.4f}")
print(f"Precision: {precision_lr_tuned:.4f}")
print(f"Recall: {recall_lr_tuned:.4f}")
print(f"F1-score: {f1_lr_tuned:.4f}")
print("Confusion Matrix:")
disp_lr_tuned = ConfusionMatrixDisplay(confusion_matrix=cm_lr_tuned)
disp_lr_tuned.plot()
plt.title('Logistic Regression Confusion Matrix (Tuned Threshold)')
plt.show()

"""## Next Steps: Calibration Check

### Subtask:
Assess the calibration of the best performing model (Logistic Regression) to understand how well the predicted probabilities align with the actual outcomes.
"""

from sklearn.calibration import calibration_curve, CalibratedClassifierCV
from sklearn.metrics import brier_score_loss
import matplotlib.pyplot as plt

# Get predicted probabilities for the positive class
y_prob_lr = best_lr_model.predict_proba(X_test)[:, 1]

# Plot calibration curve
plt.figure(figsize=(10, 10))
ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)
ax2 = plt.subplot2grid((3, 1), (2, 0))

ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")

fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_prob_lr, n_bins=10)

ax1.plot(mean_predicted_value, fraction_of_positives, "s-", label="Logistic Regression")
ax2.hist(y_prob_lr, range=(0, 1), bins=10, label="Logistic Regression", histtype="step", lw=2)

ax1.set_ylabel("Fraction of positives")
ax1.set_ylim([-0.05, 1.05])
ax1.legend(loc="lower right")
ax1.set_title('Calibration plots (Reliability Curve)')

ax2.set_xlabel("Mean predicted value")
ax2.set_ylabel("Count")
ax2.legend(loc="upper center", ncol=2)

plt.tight_layout()
plt.show()

# Calculate Brier score
brier_score_lr = brier_score_loss(y_test, y_prob_lr)
print(f"\nBrier Score for Logistic Regression: {brier_score_lr:.4f}")

# Interpret the results: A well-calibrated model's reliability curve should be close to the diagonal line.
# A lower Brier score indicates better calibration.
print("\nInterpretation:")
print("The calibration plot shows how well the predicted probabilities match the actual observed frequencies.")
print("A perfectly calibrated model's points would lie on the diagonal line.")
print("The Brier Score quantifies the accuracy of probabilistic predictions. Lower values are better.")

"""## Next Steps: Cross-Validated Stability

### Subtask:
Perform repeated stratified K-fold cross-validation to evaluate the performance variance of the models.
"""

from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score

# Define the cross-validation strategy
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)

# Evaluate Logistic Regression
scores_lr = cross_val_score(best_lr_model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)
print("Logistic Regression ROC-AUC scores from cross-validation:")
display(scores_lr)
print(f"Mean ROC-AUC: {scores_lr.mean():.4f}")
print(f"Standard Deviation of ROC-AUC: {scores_lr.std():.4f}")

# Evaluate SVM
# Note: For SVM with 'linear' kernel, cross_val_score with 'roc_auc' should work as decision_function is available.
scores_svm = cross_val_score(best_svm_model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)
print("\nSVM ROC-AUC scores from cross-validation:")
display(scores_svm)
print(f"Mean ROC-AUC: {scores_svm.mean():.4f}")
print(f"Standard Deviation of ROC-AUC: {scores_svm.std():.4f}")


# Evaluate XGBoost
scores_xgb = cross_val_score(best_xgb_model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)
print("\nXGBoost ROC-AUC scores from cross-validation:")
display(scores_xgb)
print(f"Mean ROC-AUC: {scores_xgb.mean():.4f}")
print(f"Standard Deviation of ROC-AUC: {scores_xgb.std():.4f}")

# Evaluate RandomForestClassifier
scores_rf = cross_val_score(best_rf_model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)
print("\nRandomForestClassifier ROC-AUC scores from cross-validation:")
display(scores_rf)
print(f"Mean ROC-AUC: {scores_rf.mean():.4f}")
print(f"Standard Deviation of ROC-AUC: {scores_rf.std():.4f}")

# Evaluate KNeighborsClassifier
scores_knn = cross_val_score(best_knn_model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)
print("\nKNeighborsClassifier ROC-AUC scores from cross-validation:")
display(scores_knn)
print(f"Mean ROC-AUC: {scores_knn.mean():.4f}")
print(f"Standard Deviation of ROC-AUC: {scores_knn.std():.4f}")

# Evaluate LGBMClassifier
scores_lgbm = cross_val_score(best_lgbm_model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)
print("\nLGBMClassifier ROC-AUC scores from cross-validation:")
display(scores_lgbm)
print(f"Mean ROC-AUC: {scores_lgbm.mean():.4f}")
print(f"Standard Deviation of ROC-AUC: {scores_lgbm.std():.4f}")

print("\nInterpretation:")
print("Cross-validation provides a more reliable estimate of model performance on unseen data.")
print("The mean ROC-AUC indicates the average performance, and the standard deviation indicates the variability of performance across different folds.")
print("Lower standard deviation suggests a more stable model.")

"""## Next Steps: Confounder Check & Feature Interactions

### Subtask:
Examine multicollinearity among features and explore potential feature interactions.
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
import pandas as pd
import numpy as np

# Create a copy of the processed DataFrame (X_train_processed)
# Calculate VIF for each feature in the processed DataFrame (excluding the target)
# It's important to check multicollinearity on the features used for training.
X_train_processed = X_train.copy() # Use X_train for VIF calculation

# Identify and remove one redundant dummy variable for each original categorical feature
# to address perfect multicollinearity (VIF = inf).
# Original categorical features were 'ChestPain', 'Ca', and 'Thal'.
# We'll remove one of the one-hot encoded columns for each of these.
# For example, remove 'ChestPain_typical', 'Ca_3.0', and 'Thal_reversable'.
# The choice of which one to remove is arbitrary, as long as one from each original feature is removed.

# Corrected: 'Ca_3.0' is not a valid column as 'Ca' was not one-hot encoded.
# Only drop columns that are actually present after get_dummies with drop_first=True.
columns_to_drop_vif = ['ChestPain_typical', 'Thal_reversable']

# Safely filter columns to drop to ensure they exist in the DataFrame
columns_to_drop_vif = [col for col in columns_to_drop_vif if col in X_train_processed.columns]

X_train_processed_vif = X_train_processed.drop(columns=columns_to_drop_vif)

# Convert boolean columns to integers (0 or 1) for VIF calculation
# This addresses the TypeError: ufunc 'isfinite' not supported for boolean inputs
X_train_processed_vif = X_train_processed_vif.astype(float)

vif_data = pd.DataFrame()
vif_data["feature"] = X_train_processed_vif.columns

# Calculating VIF for each feature
# Handle potential division by zero if a feature has zero variance (shouldn't happen with scaled data usually)
# Use .values to avoid issues with pandas indexing in statsmodels VIF calculation
vif_data["VIF"] = [variance_inflation_factor(X_train_processed_vif.values, i)
                   for i in range(X_train_processed_vif.shape[1])]

print("--- Variance Inflation Factor (VIF) ---")
print("VIF measures multicollinearity among features. A VIF > 5 or 10 is often considered problematic.")
display(vif_data.sort_values(by="VIF", ascending=False))

print("\nInterpretation of VIF:")
print("- VIF values indicate how much the variance of the estimated regression coefficient is increased due to multicollinearity.")
print("- High VIF values suggest that the feature is highly correlated with other features.")
print("- Decision on handling high VIF depends on the context (e.g., removing features, combining features).")

# Explore Feature Interactions (conceptual or based on domain knowledge)
print("\n--- Exploring Potential Feature Interactions ---")
print("Identifying feature interactions often requires domain knowledge or is done through experimentation.")
print("For example, the interaction between 'Age' and 'Sex' might be relevant, or between 'ChestPain' and 'ExAng'.")
print("Statistical methods (e.g., including interaction terms in a model and checking their significance) can also be used.")
print("\nPotential interactions to consider based on domain knowledge or initial EDA:")
print("- Age * Sex")
print("- ChestPain * ExAng")
print("- Oldpeak * Slope")
print("\nFurther steps would involve adding interaction terms to the model and re-evaluating performance and significance.")

from sklearn.ensemble import VotingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
import lightgbm as lgb
from sklearn.metrics import classification_report

# 1. Define your best individual models (use the params you found in GridSearchCV)
# Re-instantiate with best parameters from grid search
best_lr = LogisticRegression(**grid_search_lr.best_params_, random_state=42, solver='liblinear') # Explicitly add solver for LogisticRegression
best_svm = SVC(**grid_search_svm.best_params_, probability=True, random_state=42) # probability=True is required!
best_rf = RandomForestClassifier(**grid_search_rf.best_params_, random_state=42)
best_knn = KNeighborsClassifier(**grid_search_knn.best_params_)
best_lgbm = lgb.LGBMClassifier(**grid_search_lgbm.best_params_, random_state=42)

# 2. Create the Stacking Ensemble (Research Standard)
# ---------------------------------------------------
# Stacking uses a "final estimator" to learn how to best combine the base models.
# This is scientifically superior to simple Voting.

stacking_clf = StackingClassifier(
    estimators=[
        ('lr', best_lr),
        ('svm', best_svm),
        ('rf', best_rf),
        ('knn', best_knn),
        ('lgbm', best_lgbm)
    ],
    final_estimator=LogisticRegression(),  # The "Meta-Learner"
    cv=5  # 5-fold cross-validation for training the meta-learner
)

# 3. Train and Evaluate
stacking_clf.fit(X_train, y_train)
y_pred_stack = stacking_clf.predict(X_test)

print("\n--- STACKING ENSEMBLE RESULTS ---")
print(classification_report(y_test, y_pred_stack))



import joblib

# Define the filename for your saved model
model_filename = 'stacking_heart_disease_model.joblib'

# Save the stacking ensemble model
joblib.dump(stacking_model, model_filename)

print(f"Model successfully saved to '{model_filename}'")

"""You can load the model back into your application using `joblib.load()`:"""

import joblib
import pandas as pd

# Define the filename of your saved model
model_filename = 'stacking_heart_disease_model.joblib'

# Load the model from the file
loaded_model = joblib.load(model_filename)

print(f"Model successfully loaded from '{model_filename}'")

# Example of using the loaded model for prediction
# Assuming you have new_data (a pandas DataFrame, preprocessed like X_test_scaled_df)
# For demonstration, let's use X_test_scaled_df as 'new_data'

# Ensure the loaded model is ready to predict
# If X_test_scaled_df was used for saving, it should be used for loading as well
# Or, if you need to pass raw data, make sure it goes through the same preprocessing steps
# (scaling, one-hot encoding, etc.) that X_test_scaled_df went through.

# To predict on new data, it needs to be preprocessed exactly like the training data.
# For example, using the X_test_scaled_df that we already have.
predictions_on_new_data = loaded_model.predict(X_test_scaled_df)

print("\nExample predictions on a portion of the test set:")
print(predictions_on_new_data[:10])
print("\nActual labels for the same portion:")
print(y_test.values[:10])

from sklearn.metrics import classification_report, confusion_matrix

# Assuming loaded_model is available from the previous cell
# and X_test_scaled_df, y_test are also available.

print("\n--- Testing Loaded Model on Test Set ---")

y_pred_loaded = loaded_model.predict(X_test_scaled_df)

print("\nClassification Report for Loaded Model:")
print(classification_report(y_test, y_pred_loaded))

print("\nConfusion Matrix for Loaded Model:")
print(confusion_matrix(y_test, y_pred_loaded))



# --- INSTALL OPTUNA IF NEEDED ---
# !pip install optuna

import optuna
import lightgbm as lgb
from sklearn.metrics import accuracy_score

def objective(trial):
    # Search space based on Paper 2 recommendations
    param = {
        'objective': 'binary',
        'metric': 'binary_logloss',
        'verbosity': -1,
        'boosting_type': 'gbdt',
        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
        'num_leaves': trial.suggest_int('num_leaves', 2, 256),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
    }

    # Train LGBM with suggested params
    model = lgb.LGBMClassifier(**param)
    model.fit(X_train_bal, y_train_bal)
    preds = model.predict(X_test_scaled)
    return accuracy_score(y_test, preds)

# Run Optimization
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

print('Best trial:', study.best_trial.params)
best_lgbm = lgb.LGBMClassifier(**study.best_trial.params)

from sklearn.neural_network import MLPClassifier

# 1. Define the Neural Network (mimicking Paper 4's Dense Layers)
mlp_model = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=500, random_state=42)

# 2. Update your Stacking Classifier
# Add 'mlp' to estimators and use 'best_lgbm' from Optuna
stacking_model = StackingClassifier(
    estimators=[
        ('lr', LogisticRegression(max_iter=1000)),
        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
        ('svm', SVC(probability=True, kernel='linear')), # Added kernel='linear' for consistency or as found by GridSearchCV if used on SVM
        ('lgbm', best_lgbm),  # Your Optuna-tuned model
        ('mlp', mlp_model)    # NEW: Neural Network component
    ],
    final_estimator=LogisticRegression(),
    cv=5
)

# 3. Train
stacking_model.fit(X_train_bal, y_train_bal)

import joblib

# Extract the fitted MLP model from the stacking_model
# The base estimators are stored in the 'named_estimators_' dictionary after fitting.
# We need to find the one named 'mlp'.
for name, estimator in stacking_model.named_estimators_.items():
    if name == 'mlp':
        fitted_mlp_model = estimator
        break

if 'fitted_mlp_model' in locals():
    model_filename_mlp = 'fitted_mlp_model.joblib'
    joblib.dump(fitted_mlp_model, model_filename_mlp)
    print(f"Fitted MLP model successfully saved to '{model_filename_mlp}'")
else:
    print("Error: Could not find the 'mlp' estimator in the fitted stacking model.")

# You can load it back using:
# loaded_mlp = joblib.load('fitted_mlp_model.joblib')



import joblib
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd

# Define the filename of the saved MLP model
model_filename_mlp = 'fitted_mlp_model.joblib'

# Load the MLP model from the file
loaded_mlp = joblib.load(model_filename_mlp)

print(f"MLP model successfully loaded from '{model_filename_mlp}'")

# Ensure X_test_scaled_df is available and properly formatted for prediction
# The MLP model was trained on X_train_bal (numpy array), so we'll convert X_test_scaled_df to values.
# This ensures consistency as the MLP was trained as a base estimator within StackingClassifier
# which was fed numpy arrays from X_train_bal.

y_pred_mlp = loaded_mlp.predict(X_test_scaled_df.values)

print("\n--- Testing Loaded MLP Model on Test Set ---")
print("\nClassification Report for Loaded MLP Model:")
print(classification_report(y_test, y_pred_mlp))

print("\nConfusion Matrix for Loaded MLP Model:")
print(confusion_matrix(y_test, y_pred_mlp))

